{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/swdata/yin/miniconda3/envs/prosody/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde, entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils.approximation import cross_validate_gkde_bandwidth\n",
    "from src.utils.approximation import monte_carlo_diff_entropy\n",
    "from src.data.f0_regression_datamodule import (\n",
    "    F0RegressionDataModule as DataModule,\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "def assign_labels(input_string, labels):\n",
    "    # Create list to hold words and punctuation\n",
    "    words_with_punctuation = re.findall(r'[\\u0E00-\\u0E7F\\w]+|[.,!?;\"-]|\\'', input_string) #words_with_punctuation = re.findall(r\"[\\w']+|[.,!?;\\\"-]|'\", input_string)\n",
    "\n",
    "    # Create list to hold only words\n",
    "    #words_only = re.findall(r\"\\w+'?\\w*\", input_string) #original\n",
    "    words_only= re.findall(r'[\\u0E00-\\u0E7F\\w]+', input_string) #gio\n",
    "\n",
    "\n",
    "    # Make sure the number of labels matches the number of words\n",
    "    if not len(labels) == len(words_only):\n",
    "        # alignmend or extraction failed, skip sample\n",
    "        return None, None, None\n",
    "\n",
    "    # Create a generator for word-label pairs\n",
    "    word_label_pairs = ((word, label) for word, label in zip(words_only, labels))\n",
    "\n",
    "    # Create list of tuples where each word is matched to a label and each punctuation is matched to None\n",
    "    words_with_labels = []\n",
    "    for token in words_with_punctuation:\n",
    "        #print(token)\n",
    "        if re.match(r'[\\u0E00-\\u0E7F\\w]+', token): # original: if re.match(r\"\\w+'?\\w*\", token):\n",
    "            #print(\"match\")\n",
    "            words_with_labels.append(next(word_label_pairs))\n",
    "        else:\n",
    "            words_with_labels.append((token, None))\n",
    "            #print(\"no match\")\n",
    "\n",
    "    return words_only, words_with_punctuation, words_with_labels\n",
    "\n",
    "\n",
    "def assign_labels_to_sentences(sentences, labels):\n",
    "    single_words = []\n",
    "    single_labels = []\n",
    "    for i in range(len(sentences)):\n",
    "        words_only, words_with_punct, words_with_labels = assign_labels(\n",
    "            sentences[i], labels[i]\n",
    "        )\n",
    "        # check if alignment failed\n",
    "        if words_only is None:\n",
    "            #print(f\"Alignment failed for sentence {i}\")\n",
    "            continue\n",
    "        #print(words_with_labels)\n",
    "        # remove Nones\n",
    "        words_with_labels = [(w, l) for w, l in words_with_labels if l is not None]\n",
    "        if len(words_with_labels) == 0:\n",
    "            #print(\"No labels for sentence {i}\")\n",
    "            continue\n",
    "        # process words and labels\n",
    "        words, word_labels = zip(\n",
    "            *[(w, l) for w, l in words_with_labels if l is not None]\n",
    "        )\n",
    "        single_words.extend(words)\n",
    "        single_labels.extend(word_labels)\n",
    "        \n",
    "\n",
    "    return single_words, single_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_gkde_bandwidth(data, bandwidths = [\"scott\", \"silverman\", 0.01, 0.1, 0.3]):\n",
    "    # Cross-validation setup\n",
    "    n_splits = 5\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    split_size = len(data) // n_splits\n",
    "    splits = [indices[i*split_size:(i+1)*split_size] for i in range(n_splits)]\n",
    "    log_likelihoods = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for bw in bandwidths:\n",
    "        fold_log_likelihoods = []\n",
    "        for i in range(n_splits):\n",
    "            test_indices = splits[i]\n",
    "            train_indices = np.concatenate([splits[j] for j in range(n_splits) if j != i])\n",
    "            \n",
    "            train_data = data[train_indices]\n",
    "            test_data = data[test_indices]\n",
    "            \n",
    "            kde = gaussian_kde(train_data.T, bw_method=bw)\n",
    "            log_likelihood = np.sum(np.log(kde.evaluate(test_data.T)))\n",
    "            fold_log_likelihoods.append(log_likelihood)\n",
    "            \n",
    "        log_likelihoods.append(np.mean(fold_log_likelihoods))\n",
    "\n",
    "    # Select the best bandwidth\n",
    "    best_bandwidth = bandwidths[np.argmax(log_likelihoods)]\n",
    "    # print(f\"Optimal bandwidth: {best_bandwidth}\")\n",
    "    \n",
    "    return best_bandwidth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cond_entropy(word_label_dict, test_word_label_dict, uncond_density, num_labels_thres=20, esp=1e-7):\n",
    "    word_entropy_dict = {}\n",
    "    word_numoflabels_dict = {}\n",
    "\n",
    "    for word, label in word_label_dict.items():\n",
    "        label = np.array(label)\n",
    "\n",
    "        if word in test_word_label_dict:\n",
    "            test_labels = np.array(test_word_label_dict[word])\n",
    "\n",
    "            # If a word has enough samples, estimate KDE normally\n",
    "            if len(label) > num_labels_thres:\n",
    "                best_bw = select_gkde_bandwidth(label, bandwidths=[\"scott\", \"silverman\", 0.3])\n",
    "                # best_bw = \"scott\"\n",
    "                density = gaussian_kde(label.T, bw_method=best_bw)\n",
    "            else:\n",
    "                # Use the unconditional KDE if the word is rare\n",
    "                density = uncond_density  \n",
    "\n",
    "            # Compute entropy\n",
    "            my_entropy = -np.mean(np.log(density(test_labels.T) + esp))\n",
    "            word_entropy_dict[word] = my_entropy\n",
    "            word_numoflabels_dict[word] = len(test_labels)\n",
    "    \n",
    "    total_numoflabels = sum(word_numoflabels_dict.values())\n",
    "    cond_entropy = sum([word_numoflabels_dict[word] / total_numoflabels * word_entropy_dict[word] for word in word_entropy_dict])\n",
    "\n",
    "    return cond_entropy, word_entropy_dict\n",
    "\n",
    "\n",
    "def merge_labels(*labels):\n",
    "    return np.concatenate(labels, axis=0)\n",
    "\n",
    "    # Combine the train, test and dev dictionaries\n",
    "def merge_dicts(*dicts):\n",
    "    result = {}\n",
    "    for d in dicts:\n",
    "        for key, value in d.items():\n",
    "            if key in result:\n",
    "                result[key] = np.concatenate((result[key], value))\n",
    "            else:\n",
    "                result[key] = value\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cond Entropy with Bootstrap and Set PMI = 0 for Rare Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate mutual information with the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# set seed\n",
    "np.random.seed(234)\n",
    "esp = 1e-7\n",
    "for num_labels_thres in [20, 30, 40, 50, 60]:\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    parameters = 4\n",
    "    mode = 'dct'\n",
    "    SAVE_DIR = f\"/home/user/ding/Projects/Prosody/precomputed/without_llm/f0_{mode}_{parameters}\"\n",
    "    result_file = f'{SAVE_DIR}/pmi0_C-KDE-all-Thres{num_labels_thres}-{datetime_str}.txt'\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f'lang\\tcond_entropy\\tcond_entropy_std\\n')\n",
    "\n",
    "\n",
    "    languages = ['de', 'en', 'fr', 'it', 'ja', 'sv', 'th', 'vi', 'zh', 'yue', 'zh-by-char', 'yue-by-char']\n",
    "\n",
    "    n_iterations = 10  # Number of bootstrap samples\n",
    "\n",
    "\n",
    "    for lang in languages:\n",
    "        print(f'-----------------------------------Language: {lang}--------------------------------------')\n",
    "        \n",
    "        LAB_ROOT = f\"/home/user/ding/Projects/Prosody/languages/{lang}/aligned\"\n",
    "        PHONEME_LAB_ROOT = f\"/home/user/ding/Projects/Prosody/languages/{lang}/aligned\"\n",
    "        WAV_ROOT = f\"/home/user/ding/Projects/Prosody/languages/{lang}/wav_files\"\n",
    "        DATA_CACHE = f\"/home/user/ding/Projects/Prosody/languages/{lang}/cache\"\n",
    "\n",
    "        TRAIN_FILE = \"train-clean-100\"\n",
    "        VAL_FILE = \"dev-clean\"\n",
    "        TEST_FILE = \"test-clean\"\n",
    "\n",
    "        dm = DataModule(\n",
    "            wav_root=WAV_ROOT,\n",
    "            lab_root=LAB_ROOT,\n",
    "            phoneme_lab_root=PHONEME_LAB_ROOT,\n",
    "            data_cache=DATA_CACHE,\n",
    "            train_file=TRAIN_FILE,\n",
    "            val_file=VAL_FILE,\n",
    "            test_file=TEST_FILE,\n",
    "            dataset_name=f\"common_voice_{lang}\",\n",
    "            model_name=\"ai-forever/mGPT\",\n",
    "            # model_name = \"bert-base-multilingual-cased\",\n",
    "            f0_mode=mode,\n",
    "            f0_n_coeffs=parameters,\n",
    "            score_last_token=True,\n",
    "        )\n",
    "\n",
    "        dm.setup()\n",
    "\n",
    "        train_texts, train_labels = dm.train_texts, dm.train_durations\n",
    "        val_texts, val_labels = dm.val_texts, dm.val_durations\n",
    "        test_texts, test_labels = dm.test_texts, dm.test_durations\n",
    "\n",
    "        # print(f\"Lengths of train, val, test in samples: {len(train_texts), len(val_texts), len(test_texts)}\")\n",
    "        # print(test_texts)\n",
    "        # print(test_labels)\n",
    "\n",
    "        all_train_words, all_train_labels = assign_labels_to_sentences(\n",
    "            train_texts, train_labels\n",
    "        )\n",
    "        all_dev_words, all_dev_labels = assign_labels_to_sentences(val_texts, val_labels)\n",
    "        all_test_words, all_test_labels = assign_labels_to_sentences(test_texts, test_labels)\n",
    "\n",
    "        all_train_labels = np.array(all_train_labels)\n",
    "        all_dev_labels = np.array(all_dev_labels)\n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "\n",
    "        all_labels = merge_labels(all_train_labels, all_dev_labels, all_test_labels)\n",
    "        print(all_train_labels.shape)\n",
    "        print(all_labels.shape)\n",
    "        # Find different labels corresponding to the same words\n",
    "        train_word_label_dict = {}\n",
    "        for word, label in zip(all_train_words, all_train_labels):\n",
    "            word = word.lower()\n",
    "            if word not in train_word_label_dict: \n",
    "                train_word_label_dict[word] = []\n",
    "            train_word_label_dict[word].append(label)\n",
    "\n",
    "        # print(train_word_label_dict['the'])\n",
    "\n",
    "        test_word_label_dict = {}\n",
    "        for word, label in zip(all_test_words, all_test_labels):\n",
    "            word = word.lower()\n",
    "            if word not in test_word_label_dict: \n",
    "                test_word_label_dict[word] = []\n",
    "            test_word_label_dict[word].append(label)\n",
    "\n",
    "        dev_word_label_dict = {}\n",
    "        for word, label in zip(all_dev_words, all_dev_labels):\n",
    "            word = word.lower()\n",
    "            if word not in dev_word_label_dict: \n",
    "                dev_word_label_dict[word] = []\n",
    "            dev_word_label_dict[word].append(label)\n",
    "\n",
    "        #################### Check the data: Find the number of labels for each word in the datasets ######################\n",
    "        # # print the number of labels in a descending order\n",
    "        # train_word_numoflabels_dict = {k:len(v) for k, v in train_word_label_dict.items()}\n",
    "        # sorted_train_word_numoflabels_dict = sorted(train_word_numoflabels_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "        # # print(sorted_train_word_numoflabels_dict)\n",
    "        # selected_train_labels = np.concatenate([label for word, label in train_word_label_dict.items() if len(label) > num_labels_thres])\n",
    "\n",
    "        # test_word_numoflabels_dict = {k:len(v) for k, v in test_word_label_dict.items()}\n",
    "        # sorted_test_word_numoflabels_dict = sorted(test_word_numoflabels_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "        # selected_test_labels = np.concatenate([label for word, label in test_word_label_dict.items() if len(label) > num_labels_thres])\n",
    "        # selected_test_word_label_dict = {word: label for word, label in test_word_label_dict.items() if len(label) > num_labels_thres}\n",
    "        # # print(sorted_test_word_numoflabels_dict)\n",
    "        # # print(\"haha\", selected_test_word_label_dict)\n",
    "\n",
    "        # dev_word_numoflabels_dict = {k:len(v) for k, v in dev_word_label_dict.items()}\n",
    "        # sorted_dev_word_numoflabels_dict = sorted(dev_word_numoflabels_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "        # # print(sorted_dev_word_numoflabels_dict)\n",
    "        # selected_dev_labels = np.concatenate([label for word, label in dev_word_label_dict.items() if len(label) > num_labels_thres])\n",
    "\n",
    "        all_word_label_dict = merge_dicts(train_word_label_dict, dev_word_label_dict, test_word_label_dict)\n",
    "        all_word_numoflabels_dict = {k:len(v) for k, v in all_word_label_dict.items()}\n",
    "        sorted_all_word_numoflabels_dict = sorted(all_word_numoflabels_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "        selected_word_label_dict_all = {}\n",
    "        for word, labels in all_word_label_dict.items():\n",
    "            labels = np.array(labels)\n",
    "            np.random.shuffle(labels)\n",
    "            selected_word_label_dict_all[word] = labels\n",
    "            selected_labels_all = np.concatenate(list(selected_word_label_dict_all.values()))\n",
    "        ###################################################################################################################\n",
    "\n",
    "        cond_entropy_list = []\n",
    "        best_bwth = select_gkde_bandwidth(selected_labels_all, bandwidths=[\"scott\", \"silverman\", 0.3])\n",
    "        uncond_density = gaussian_kde(selected_labels_all.T, bw_method=best_bwth)\n",
    "        for i in range(n_iterations):\n",
    "            sampled_all_word_label_dict = {word: resample(labels, replace=True, n_samples=len(labels)) for word, labels in selected_word_label_dict_all.items()}\n",
    "            # conditional entropy\n",
    "            sampled_cond_entropy, _ = calculate_cond_entropy(sampled_all_word_label_dict, sampled_all_word_label_dict, uncond_density, num_labels_thres=num_labels_thres, esp=0)\n",
    "            cond_entropy_list.append(sampled_cond_entropy)\n",
    "            \n",
    "            print(f\"Iteration {i+1}/{n_iterations}: cond entropy = {sampled_cond_entropy}\")\n",
    "        \n",
    "        cond_entropy_list = np.array(cond_entropy_list)\n",
    "        cond_entropy_mean = np.mean(cond_entropy_list)\n",
    "        cond_entropy_std = np.std(cond_entropy_list)\n",
    "        \n",
    "        print(f\"Final results: Cond entropy = {cond_entropy_mean}, Cond entropy Std = {cond_entropy_std}\")\n",
    "        \n",
    "        with open(result_file, 'a') as f:\n",
    "            f.write(f'{lang}\\t{cond_entropy_mean}\\t{cond_entropy_std}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate mutual information with training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# set seed\n",
    "np.random.seed(234)\n",
    "esp = 1e-7\n",
    "for num_labels_thres in [20, 30, 40, 50, 60]:\n",
    "# for num_labels_thres in [20]:\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    parameters = 4\n",
    "    mode = 'dct'\n",
    "    SAVE_DIR = f\"/swdata/yin/Cui/prosody/notebooks/precomputed/without_llm\"\n",
    "    result_file = f'{SAVE_DIR}/pmi0_C-KDE-split-Thres{num_labels_thres}-{datetime_str}.txt'\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(f'lang\\tcond_entropy\\tcond_entropy_std\\n')\n",
    "\n",
    "\n",
    "    languages = ['de', 'en', 'fr', 'it', 'ja', 'sv', 'th', 'vi', 'zh', 'yue', 'zh-by-char', 'yue-by-char']\n",
    "\n",
    "    n_iterations = 10  # Number of bootstrap samples\n",
    "\n",
    "\n",
    "    for lang in languages:\n",
    "        print(f'-----------------------------------Language: {lang}--------------------------------------')\n",
    "        \n",
    "        LAB_ROOT = f\"/swdata/yin/Cui/prosody/languages/{lang}/aligned\"\n",
    "        PHONEME_LAB_ROOT = f\"/swdata/yin/Cui/prosody/languages/{lang}/aligned\"\n",
    "        WAV_ROOT = f\"/swdata/yin/Cui/prosody/languages/{lang}/wav_files\"\n",
    "        DATA_CACHE = f\"/swdata/yin/Cui/prosody/languages/{lang}/cache\"\n",
    "\n",
    "        TRAIN_FILE = \"train-clean-100\"\n",
    "        VAL_FILE = \"dev-clean\"\n",
    "        TEST_FILE = \"test-clean\"\n",
    "\n",
    "        dm = DataModule(\n",
    "            wav_root=WAV_ROOT,\n",
    "            lab_root=LAB_ROOT,\n",
    "            phoneme_lab_root=PHONEME_LAB_ROOT,\n",
    "            data_cache=DATA_CACHE,\n",
    "            train_file=TRAIN_FILE,\n",
    "            val_file=VAL_FILE,\n",
    "            test_file=TEST_FILE,\n",
    "            dataset_name=f\"common_voice_{lang}\",\n",
    "            model_name=\"ai-forever/mGPT\",\n",
    "            f0_mode=mode,\n",
    "            f0_n_coeffs=parameters,\n",
    "            score_last_token=True,\n",
    "        )\n",
    "\n",
    "        dm.setup()\n",
    "\n",
    "        train_texts, train_labels = dm.train_texts, dm.train_durations\n",
    "        val_texts, val_labels = dm.val_texts, dm.val_durations\n",
    "        test_texts, test_labels = dm.test_texts, dm.test_durations\n",
    "\n",
    "        # print(f\"Lengths of train, val, test in samples: {len(train_texts), len(val_texts), len(test_texts)}\")\n",
    "        # print(test_texts)\n",
    "        # print(test_labels)\n",
    "\n",
    "        all_train_words, all_train_labels = assign_labels_to_sentences(\n",
    "            train_texts, train_labels\n",
    "        )\n",
    "        all_dev_words, all_dev_labels = assign_labels_to_sentences(val_texts, val_labels)\n",
    "        all_test_words, all_test_labels = assign_labels_to_sentences(test_texts, test_labels)\n",
    "\n",
    "        all_train_labels = np.array(all_train_labels)\n",
    "        all_dev_labels = np.array(all_dev_labels)\n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "\n",
    "        all_labels = merge_labels(all_train_labels, all_dev_labels, all_test_labels)\n",
    "        print(all_train_labels.shape)\n",
    "        print(all_labels.shape)\n",
    "        # Find different labels corresponding to the same words\n",
    "        train_word_label_dict = {}\n",
    "        for word, label in zip(all_train_words, all_train_labels):\n",
    "            word = word.lower()\n",
    "            if word not in train_word_label_dict: \n",
    "                train_word_label_dict[word] = []\n",
    "            train_word_label_dict[word].append(label)\n",
    "\n",
    "        # print(train_word_label_dict['the'])\n",
    "\n",
    "        test_word_label_dict = {}\n",
    "        for word, label in zip(all_test_words, all_test_labels):\n",
    "            word = word.lower()\n",
    "            if word not in test_word_label_dict: \n",
    "                test_word_label_dict[word] = []\n",
    "            test_word_label_dict[word].append(label)\n",
    "\n",
    "        dev_word_label_dict = {}\n",
    "        for word, label in zip(all_dev_words, all_dev_labels):\n",
    "            word = word.lower()\n",
    "            if word not in dev_word_label_dict: \n",
    "                dev_word_label_dict[word] = []\n",
    "            dev_word_label_dict[word].append(label)\n",
    "\n",
    "        all_word_label_dict = merge_dicts(train_word_label_dict, dev_word_label_dict, test_word_label_dict)\n",
    "        all_word_numoflabels_dict = {k:len(v) for k, v in all_word_label_dict.items()}\n",
    "        sorted_all_word_numoflabels_dict = sorted(all_word_numoflabels_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        cond_entropy_list = []\n",
    "        # Monte Carlo estimation of differential entropy and conditional entropy\n",
    "        selected_word_label_dict_train = {}\n",
    "        selected_word_label_dict_test = {}\n",
    "        selected_word_label_dict_all = {}\n",
    "        for word, labels in all_word_label_dict.items():\n",
    "            labels = np.array(labels) \n",
    "            np.random.shuffle(labels)\n",
    "            selected_word_label_dict_all[word] = labels\n",
    "        selected_labels_all = np.concatenate(list(selected_word_label_dict_all.values()))\n",
    "        for i in range(n_iterations):\n",
    "            sampled_train_word_label_dict = {word: resample(labels, replace=True, n_samples=max(1, int(len(labels)*0.7))) for word, labels in selected_word_label_dict_all.items()}\n",
    "            sampled_test_word_label_dict = {word: resample(labels, replace=True, n_samples=max(1, int(len(labels)*0.3))) for word, labels in selected_word_label_dict_all.items()}\n",
    "            sampled_train_labels = np.concatenate(list(sampled_train_word_label_dict.values()))\n",
    "            # split all the labels into training and testing\n",
    "            best_bwth = select_gkde_bandwidth(sampled_train_labels, bandwidths=[\"scott\", \"silverman\", 0.3])\n",
    "            # print(f\"Optimal bandwidth diff: {best_bwth}\")\n",
    "            uncond_density = gaussian_kde(sampled_train_labels.T, bw_method=best_bwth) # density = gaussian_kde(selected_labels_test.T, bw_method=best_bwth) if we want to use the testing data\n",
    "            # conditional entropy\n",
    "            sampled_cond_entropy, _ = calculate_cond_entropy(sampled_train_word_label_dict, sampled_test_word_label_dict, uncond_density, num_labels_thres=num_labels_thres, esp=esp)\n",
    "            cond_entropy_list.append(sampled_cond_entropy)\n",
    "            \n",
    "            print(f\"Iteration {i+1}/{n_iterations}: cond entropy = {sampled_cond_entropy}\")\n",
    "        \n",
    "        cond_entropy_list = np.array(cond_entropy_list)\n",
    "        cond_entropy_mean = np.mean(cond_entropy_list)\n",
    "        cond_entropy_std = np.std(cond_entropy_list)\n",
    "        \n",
    "        print(f\"Final results: Cond entropy = {cond_entropy_mean}, Cond entropy Std = {cond_entropy_std}\")\n",
    "        \n",
    "        with open(result_file, 'a') as f:\n",
    "            f.write(f'{lang}\\t{cond_entropy_mean}\\t{cond_entropy_std}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
