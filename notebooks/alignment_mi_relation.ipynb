{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment and estimated mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "from src.utils.gpt2_letter_tokenizer import CustomGPT2Tokenizer, mGPTTokenizer, mBERTTokenizer, CustomBERTTokenizer\n",
    "from src.data.components.datasets import TokenTaggingDataset, tokenize_text_with_labels\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing entropy differences and their standard deviations\n",
    "diff_ent = {\n",
    "    \"de\": (10.19, 0.06),\n",
    "    \"en\": (9.49, 0.03),\n",
    "    \"fr\": (9.81, 0.06),\n",
    "    \"it\": (9.79, 0.05),\n",
    "    \"ja\": (9.25, 0.04),\n",
    "    \"kor\": (10.63, 0.19),\n",
    "    \"sr\": (7.23, 0.08),\n",
    "    \"sv\": (9.75, 0.05),\n",
    "    \"th\": (9.49, 0.07),\n",
    "    \"vi\": (10.32, 0.07),\n",
    "    \"zh\": (10.29, 0.02),\n",
    "    \"yue\": (8.70, 0.07),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mbert\"\n",
    "lang = \"yue\"\n",
    "parameters = 4 \n",
    "mode = 'dct'\n",
    "\n",
    "LAB_ROOT = f\"/home/user/ding/Projects/Prosody/languages/{lang}/aligned\"\n",
    "PHONEME_LAB_ROOT = f\"/home/user/ding/Projects/Prosody/languages/{lang}/aligned\"\n",
    "WAV_ROOT = f\"/home/user/ding/Projects/Prosody/languages/{lang}/wav_files\"\n",
    "DATA_CACHE = f\"/home/user/ding/Projects/Prosody/languages/{lang}/cache\"\n",
    "\n",
    "PRED_ROOT = f\"/home/user/ding/Projects/Prosody/logs/train/runs/{model}\"\n",
    "\n",
    "orig_model = f\"{PRED_ROOT}/{model}_{lang}_orig/metrics.json\"\n",
    "mis2_model = f\"{PRED_ROOT}/{model}_{lang}_mis1/metrics.json\" # naming is swapped\n",
    "mis1_model = f\"{PRED_ROOT}/{model}_{lang}_mis2/metrics.json\" # naming is swapped\n",
    "cus_model = f\"{PRED_ROOT}/{model}_{lang}_noletter/metrics.json\"\n",
    "# letter_model = f\"{PRED_ROOT}/{model}_{lang}_letter/metrics.json\"\n",
    "\n",
    "TRAIN_FILE = \"train-clean-100\"\n",
    "VAL_FILE = \"dev-clean\"\n",
    "TEST_FILE = \"test-clean\"\n",
    "\n",
    "SAVE_DIR = f\"/home/user/ding/Projects/Prosody/precomputed/{model}/mi_alignment_{lang}_{model}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loss(model_path):\n",
    "    with open(model_path, 'r') as file:\n",
    "        metrics_data = json.load(file)\n",
    "    return metrics_data.get(\"test/loss\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional entropy (cond_ent) for original model: 0.78\n",
      "Conditional entropy (cond_ent) for misalignment 1 model: 0.75\n",
      "Conditional entropy (cond_ent) for misalignment 2 model: 0.72\n",
      "Conditional entropy (cond_ent) for custom model: 0.76\n"
     ]
    }
   ],
   "source": [
    "test_loss_0 = get_test_loss(orig_model)\n",
    "test_loss_1 = get_test_loss(mis1_model)\n",
    "test_loss_2 = get_test_loss(mis2_model)\n",
    "test_loss_3 = get_test_loss(cus_model)\n",
    "\n",
    "mi_mis0 = diff_ent[lang][0] - test_loss_0 if test_loss_0 is not None else None\n",
    "mi_mis1 = diff_ent[lang][0] - test_loss_1 if test_loss_1 is not None else None\n",
    "mi_mis2 = diff_ent[lang][0] - test_loss_2 if test_loss_2 is not None else None\n",
    "mi_mis3 = diff_ent[lang][0] - test_loss_3 if test_loss_3 is not None else None\n",
    "\n",
    "print(f\"Conditional entropy (cond_ent) for original model: {mi_mis0:.2f}\")\n",
    "\n",
    "print(f\"Conditional entropy (cond_ent) for misalignment 1 model: {mi_mis1:.2f}\")\n",
    "\n",
    "print(f\"Conditional entropy (cond_ent) for misalignment 2 model: {mi_mis2:.2f}\")\n",
    "\n",
    "print(f\"Conditional entropy (cond_ent) for custom model: {mi_mis3:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.f0_regression_datamodule import (\n",
    "    F0RegressionDataModule as DataModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bert-base-multilingual-cased tokenizer.\n",
      "Dataloader: padding with token id: 0\n",
      "Loading data from cache: ('/home/user/ding/Projects/Prosody/languages/yue/cache/train-clean-100', 'f0_dct_4.pkl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 4012/4012 [00:01<00:00, 2819.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 369/4012\n",
      "Loading data from cache: ('/home/user/ding/Projects/Prosody/languages/yue/cache/dev-clean', 'f0_dct_4.pkl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 486/486 [00:00<00:00, 3791.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 49/486\n",
      "Loading data from cache: ('/home/user/ding/Projects/Prosody/languages/yue/cache/test-clean', 'f0_dct_4.pkl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 502/502 [00:00<00:00, 3827.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 50/502\n",
      "Train dataset size: 3643\n",
      "Validation dataset size: 437\n",
      "Test dataset size: 452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(\n",
    "    wav_root=WAV_ROOT,\n",
    "    lab_root=LAB_ROOT,\n",
    "    phoneme_lab_root=PHONEME_LAB_ROOT,\n",
    "    data_cache=DATA_CACHE,\n",
    "    train_file=TRAIN_FILE,\n",
    "    val_file=VAL_FILE,\n",
    "    test_file=TEST_FILE,\n",
    "    dataset_name=f\"CommonVoice_{lang}\",\n",
    "    model_name=\"bert-base-multilingual-cased\",\n",
    "    f0_mode=mode,\n",
    "    f0_n_coeffs=parameters,\n",
    "    score_last_token=True,\n",
    "    tokenization_by_letter=False,\n",
    ")\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of train, val, test in samples: (4012, 486, 502)\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = dm.train_texts, dm.train_durations\n",
    "val_texts, val_labels = dm.val_texts, dm.val_durations\n",
    "test_texts, test_labels = dm.test_texts, dm.test_durations\n",
    "\n",
    "print(\n",
    "    f\"Lengths of train, val, test in samples: {len(train_texts), len(val_texts), len(test_texts)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def assign_labels(input_string, labels):\n",
    "    # Create list to hold words and punctuation\n",
    "    words_with_punctuation = re.findall(r'[\\u0E00-\\u0E7F\\w]+|[.,!?;\"-]|\\'', input_string) #words_with_punctuation = re.findall(r\"[\\w']+|[.,!?;\\\"-]|'\", input_string)\n",
    "\n",
    "    # Create list to hold only words\n",
    "    #words_only = re.findall(r\"\\w+'?\\w*\", input_string) #original\n",
    "    words_only= re.findall(r'[\\u0E00-\\u0E7F\\w]+', input_string) #gio\n",
    "\n",
    "\n",
    "    # Make sure the number of labels matches the number of words\n",
    "    if not len(labels) == len(words_only):\n",
    "        # alignmend or extraction failed, skip sample\n",
    "        return None, None, None\n",
    "\n",
    "    # Create a generator for word-label pairs\n",
    "    word_label_pairs = ((word, label) for word, label in zip(words_only, labels))\n",
    "\n",
    "    # Create list of tuples where each word is matched to a label and each punctuation is matched to None\n",
    "    words_with_labels = []\n",
    "    for token in words_with_punctuation:\n",
    "        #print(token)\n",
    "        if re.match(r'[\\u0E00-\\u0E7F\\w]+', token): # original: if re.match(r\"\\w+'?\\w*\", token):\n",
    "            #print(\"match\")\n",
    "            words_with_labels.append(next(word_label_pairs))\n",
    "        else:\n",
    "            words_with_labels.append((token, None))\n",
    "            #print(\"no match\")\n",
    "\n",
    "    return words_only, words_with_punctuation, words_with_labels\n",
    "\n",
    "\n",
    "def assign_labels_to_sentences(sentences, labels):\n",
    "    single_words = []\n",
    "    single_labels = []\n",
    "    for i in range(len(sentences)):\n",
    "        words_only, words_with_punct, words_with_labels = assign_labels(\n",
    "            sentences[i], labels[i]\n",
    "        )\n",
    "        # check if alignment failed\n",
    "        if words_only is None:\n",
    "            #print(f\"Alignment failed for sentence {i}\")\n",
    "            continue\n",
    "        #print(words_with_labels)\n",
    "        # remove Nones\n",
    "        words_with_labels = [(w, l) for w, l in words_with_labels if l is not None]\n",
    "        if len(words_with_labels) == 0:\n",
    "            #print(\"No labels for sentence {i}\")\n",
    "            continue\n",
    "        # process words and labels\n",
    "        words, word_labels = zip(\n",
    "            *[(w, l) for w, l in words_with_labels if l is not None]\n",
    "        )\n",
    "        single_words.extend(words)\n",
    "        single_labels.extend(word_labels)\n",
    "        \n",
    "\n",
    "    return single_words, single_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and labels train: (30157, 30157)\n",
      "Words and labels dev: (3567, 3567)\n",
      "Words and labels test: (3656, 3656)\n",
      "Mean word length: 1.3604868913857677\n"
     ]
    }
   ],
   "source": [
    "#from src.utils.text_processing import assign_labels_to_sentences\n",
    "\n",
    "all_train_words, all_train_labels = assign_labels_to_sentences(\n",
    "    train_texts, train_labels\n",
    ")\n",
    "all_dev_words, all_dev_labels = assign_labels_to_sentences(val_texts, val_labels)\n",
    "all_test_words, all_test_labels = assign_labels_to_sentences(test_texts, test_labels)\n",
    "\n",
    "print(f\"Words and labels train: {len(all_train_words), len(all_train_labels)}\")\n",
    "print(f\"Words and labels dev: {len(all_dev_words), len(all_dev_labels)}\")\n",
    "print(f\"Words and labels test: {len(all_test_words), len(all_test_labels)}\")\n",
    "\n",
    "all_words = all_train_words + all_dev_words + all_test_words\n",
    "\n",
    "# Compute the length of each word\n",
    "word_lengths = [len(word) for word in all_words]\n",
    "\n",
    "# Compute the mean word length\n",
    "mean_word_length = sum(word_lengths) / len(word_lengths)\n",
    "\n",
    "print(f\"Mean word length: {mean_word_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract input_ids and calculate correct alignment proportion for any dataset\n",
    "def calculate_alignment_proportion(dataset):\n",
    "    total_words = 0\n",
    "    total_misalignment = 0\n",
    "    discard_count_1 = 0\n",
    "    discard_count_2 = 0\n",
    "    discard_count_3 = 0\n",
    "    \n",
    "    # Iterate through all items in the dataset\n",
    "    for i in range(len(dataset)):\n",
    "        # Get each item from the dataset\n",
    "        item = dataset.__getitem__(i)\n",
    "        \n",
    "        input_ids = item['input_ids']  \n",
    "        loss_mask = item['loss_mask']  \n",
    "        word_to_tokens = item[\"word_to_tokens\"]\n",
    "        \n",
    "        # Iterate through only the 2nd, 4th, 6th, etc., elements excluding the last two\n",
    "        for token_ids in word_to_tokens[1:-2:2]:  # Start at index 1 and step by 2, skipping last 2 elements\n",
    "            if isinstance(token_ids, list) and len(token_ids) > 1:\n",
    "                total_misalignment += 1\n",
    "                discard_count_3 += 1\n",
    "            if isinstance(token_ids, list) and len(token_ids) > 2:\n",
    "                discard_count_2 += 1\n",
    "            if isinstance(token_ids, list) and len(token_ids) > 3:\n",
    "                discard_count_1 += 1\n",
    "\n",
    "        num_words = sum(1 for mask in loss_mask if mask == 1)\n",
    "        total_words += num_words\n",
    "\n",
    "    misalignment_count_1 = total_misalignment - discard_count_1\n",
    "    misalignment_count_2 = total_misalignment - discard_count_2\n",
    "    misalignment_count_3 = total_misalignment - discard_count_3\n",
    "\n",
    "    if total_words > 0:\n",
    "        misalignment_proportion_0 = total_misalignment / total_words\n",
    "        misalignment_proportion_1 = misalignment_count_1 / total_words\n",
    "        misalignment_proportion_2 = misalignment_count_2 / total_words\n",
    "        misalignment_proportion_3 = misalignment_count_3 / total_words\n",
    "    else:\n",
    "        misalignment_proportion_0 = 0\n",
    "        misalignment_proportion_1 = 0\n",
    "        misalignment_proportion_2 = 0\n",
    "        misalignment_proportion_3 = 0\n",
    "    \n",
    "    return misalignment_proportion_0, total_misalignment, misalignment_proportion_1, misalignment_count_1, misalignment_proportion_2, misalignment_count_2, misalignment_proportion_3, misalignment_count_3, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: Total Words = 29133\n",
      "Original wrong Alignments = 7515, Original Proportion = 0.2580\n",
      "Wrong Alignments 1 = 7426, Proportion 1 = 0.2549\n",
      "Wrong Alignments 2 = 6750, Proportion 2 = 0.2317\n",
      "Wrong Alignments 3 = 0, Proportion 3 = 0.0000\n",
      "Dev Dataset: Total Words = 3431\n",
      "Original wrong Alignments = 942, Original Proportion = 0.2746\n",
      "Wrong Alignments 1 = 928, Proportion 1 = 0.2705\n",
      "Wrong Alignments 2 = 836, Proportion 2 = 0.2437\n",
      "Wrong Alignments 3 = 0, Proportion 3 = 0.0000\n",
      "Test Dataset: Total Words = 3513\n",
      "Original wrong Alignments = 944, Original Proportion = 0.2687\n",
      "Wrong Alignments 1 = 937, Proportion 1 = 0.2667\n",
      "Wrong Alignments 2 = 855, Proportion 2 = 0.2434\n",
      "Wrong Alignments 3 = 0, Proportion 3 = 0.0000\n",
      "--------------------------------------------------\n",
      "All Dataset: Total Words = 36077\n",
      "Original wrong Alignments = 9401, Original Proportion = 0.2606\n",
      "Wrong Alignments 1 = 9291, Proportion 1 = 0.2575\n",
      "Wrong Alignments 2 = 8441, Proportion 2 = 0.2340\n",
      "Wrong Alignments 3 = 0, Proportion 3 = 0.0000\n",
      "Original used words = 36077, Original used Proportion = 1.0000\n",
      "Used words 1 = 35967, Original used Proportion = 0.9970\n",
      "Used words 2 = 35117, Original used Proportion = 0.9734\n",
      "Used words 3 = 26676, Original used Proportion = 0.7394\n"
     ]
    }
   ],
   "source": [
    "train_mis_proportion_0, train_total_misalignment, train_mis_proportion_1, train_misalignment_count_1, train_mis_proportion_2, train_misalignment_count_2, train_mis_proportion_3, train_misalignment_count_3, train_total_words = calculate_alignment_proportion(dm.train_dataset)\n",
    "dev_mis_proportion_0, dev_total_misalignment, dev_mis_proportion_1, dev_misalignment_count_1, dev_mis_proportion_2, dev_misalignment_count_2, dev_mis_proportion_3, dev_misalignment_count_3, dev_total_words = calculate_alignment_proportion(dm.val_dataset)\n",
    "test_mis_proportion_0, test_total_misalignment, test_mis_proportion_1, test_misalignment_count_1, test_mis_proportion_2, test_misalignment_count_2, test_mis_proportion_3, test_misalignment_count_3,test_total_words = calculate_alignment_proportion(dm.test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "all_misalignment = train_total_misalignment + dev_total_misalignment + test_total_misalignment\n",
    "all_misalignment_count_1 = train_misalignment_count_1 + dev_misalignment_count_1 + test_misalignment_count_1\n",
    "all_misalignment_count_2 = train_misalignment_count_2 + dev_misalignment_count_2 + test_misalignment_count_2\n",
    "all_misalignment_count_3 = train_misalignment_count_3 + dev_misalignment_count_3 + test_misalignment_count_3\n",
    "\n",
    "all_total_words = train_total_words + dev_total_words + test_total_words\n",
    "\n",
    "all_mis_proportion_0 = all_misalignment / all_total_words \n",
    "all_mis_proportion_1 = all_misalignment_count_1 / all_total_words \n",
    "all_mis_proportion_2 = all_misalignment_count_2 / all_total_words \n",
    "all_mis_proportion_3 = all_misalignment_count_3 / all_total_words \n",
    "\n",
    "clean_words = all_total_words - all_misalignment\n",
    "using_words_0 = all_total_words\n",
    "using_words_1 = clean_words + all_misalignment_count_1\n",
    "using_words_2 = clean_words + all_misalignment_count_2\n",
    "using_words_3 = clean_words + all_misalignment_count_3\n",
    "\n",
    "using_words_prop_0 = using_words_0 / all_total_words\n",
    "using_words_prop_1 = using_words_1 / all_total_words\n",
    "using_words_prop_2 = using_words_2 / all_total_words\n",
    "using_words_prop_3 = using_words_3 / all_total_words\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Train Dataset: Total Words = {train_total_words}\")\n",
    "print(f\"Original wrong Alignments = {train_total_misalignment}, Original Proportion = {train_mis_proportion_0:.4f}\")\n",
    "print(f\"Wrong Alignments 1 = {train_misalignment_count_1}, Proportion 1 = {train_mis_proportion_1:.4f}\")\n",
    "print(f\"Wrong Alignments 2 = {train_misalignment_count_2}, Proportion 2 = {train_mis_proportion_2:.4f}\")\n",
    "print(f\"Wrong Alignments 3 = {train_misalignment_count_3}, Proportion 3 = {train_mis_proportion_3:.4f}\")\n",
    "print(f\"Dev Dataset: Total Words = {dev_total_words}\")\n",
    "print(f\"Original wrong Alignments = {dev_total_misalignment}, Original Proportion = {dev_mis_proportion_0:.4f}\")\n",
    "print(f\"Wrong Alignments 1 = {dev_misalignment_count_1}, Proportion 1 = {dev_mis_proportion_1:.4f}\")\n",
    "print(f\"Wrong Alignments 2 = {dev_misalignment_count_2}, Proportion 2 = {dev_mis_proportion_2:.4f}\")\n",
    "print(f\"Wrong Alignments 3 = {dev_misalignment_count_3}, Proportion 3 = {dev_mis_proportion_3:.4f}\")\n",
    "print(f\"Test Dataset: Total Words = {test_total_words}\")\n",
    "print(f\"Original wrong Alignments = {test_total_misalignment}, Original Proportion = {test_mis_proportion_0:.4f}\")\n",
    "print(f\"Wrong Alignments 1 = {test_misalignment_count_1}, Proportion 1 = {test_mis_proportion_1:.4f}\")\n",
    "print(f\"Wrong Alignments 2 = {test_misalignment_count_2}, Proportion 2 = {test_mis_proportion_2:.4f}\")\n",
    "print(f\"Wrong Alignments 3 = {test_misalignment_count_3}, Proportion 3 = {test_mis_proportion_3:.4f}\")\n",
    "\n",
    "print(\"-\" * 50) \n",
    "\n",
    "print(f\"All Dataset: Total Words = {all_total_words}\")\n",
    "print(f\"Original wrong Alignments = {all_misalignment}, Original Proportion = {all_mis_proportion_0:.4f}\")\n",
    "print(f\"Wrong Alignments 1 = {all_misalignment_count_1}, Proportion 1 = {all_mis_proportion_1:.4f}\")\n",
    "print(f\"Wrong Alignments 2 = {all_misalignment_count_2}, Proportion 2 = {all_mis_proportion_2:.4f}\")\n",
    "print(f\"Wrong Alignments 3 = {all_misalignment_count_3}, Proportion 3 = {all_mis_proportion_3:.4f}\")\n",
    "\n",
    "print(f\"Original used words = {using_words_0}, Original used Proportion = {using_words_prop_0:.4f}\")\n",
    "print(f\"Used words 1 = {using_words_1}, Original used Proportion = {using_words_prop_1:.4f}\")\n",
    "print(f\"Used words 2 = {using_words_2}, Original used Proportion = {using_words_prop_2:.4f}\")\n",
    "print(f\"Used words 3 = {using_words_3}, Original used Proportion = {using_words_prop_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def format_float(value):\n",
    "    if isinstance(value, float):\n",
    "        return f\"{value:.2f}\"\n",
    "    return value\n",
    "\n",
    "# Function to write results to a CSV file\n",
    "def write_results_to_csv(file_path, model, language, total_words, misalignment, mis_proportion_0, used_words_0, used_prop_0,\n",
    "                         misalignment_count_1, mis_proportion_1, used_words_1, used_prop_1,\n",
    "                         misalignment_count_2, mis_proportion_2, used_words_2, used_prop_2,\n",
    "                         misalignment_count_3, mis_proportion_3, used_words_3, used_prop_3,\n",
    "                         diff_entropy, test_loss_0, test_loss_1, test_loss_2, test_loss_3,\n",
    "                         mi_mis0, mi_mis1, mi_mis2, mi_mis3):\n",
    "    \n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow([\n",
    "            'model', 'lang', 'n_words', 'n_mis_orig', 'prop_mis_orig', 'n_used_orig', 'prop_used_orig',\n",
    "            'n_mis_1', 'prop_mis_1', 'n_used_1', 'prop_used_1',\n",
    "            'n_mis_2', 'prop_mis_2', 'n_used_2', 'prop_used_2',\n",
    "            'n_mis_3', 'prop_mis_3', 'n_used_3', 'prop_used_3',\n",
    "            \"diff_entropy\", \"cond_ent_0\", \"cond_ent_1\", \"cond_ent_2\", \"cond_ent_3\",\n",
    "            \"mi_0\", \"mi_1\", \"mi_2\", \"mi_3\"\n",
    "        ])\n",
    "        \n",
    "        # Write the results for the current language, formatting floats to two decimal places\n",
    "        writer.writerow([\n",
    "            model,\n",
    "            language, \n",
    "            total_words, \n",
    "            misalignment, \n",
    "            format_float(mis_proportion_0), \n",
    "            used_words_0, \n",
    "            format_float(used_prop_0),\n",
    "            misalignment_count_1, \n",
    "            format_float(mis_proportion_1), \n",
    "            used_words_1, \n",
    "            format_float(used_prop_1),\n",
    "            misalignment_count_2, \n",
    "            format_float(mis_proportion_2), \n",
    "            used_words_2, \n",
    "            format_float(used_prop_2),\n",
    "            misalignment_count_3, \n",
    "            format_float(mis_proportion_3), \n",
    "            used_words_3, \n",
    "            format_float(used_prop_3),\n",
    "            format_float(diff_entropy), \n",
    "            format_float(test_loss_0), \n",
    "            format_float(test_loss_1), \n",
    "            format_float(test_loss_2), \n",
    "            format_float(test_loss_3),\n",
    "            format_float(mi_mis0), \n",
    "            format_float(mi_mis1), \n",
    "            format_float(mi_mis2), \n",
    "            format_float(mi_mis3)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "# write_results_to_csv(SAVE_DIR, model, lang, all_total_words, all_misalignment, all_mis_proportion_0, using_words_0, using_words_prop_0,\n",
    "#                      all_misalignment_count_1, all_mis_proportion_1, using_words_1, using_words_prop_1,\n",
    "#                      all_misalignment_count_2, all_mis_proportion_2, using_words_2, using_words_prop_2,\n",
    "#                      all_misalignment_count_3, all_mis_proportion_3, using_words_3, using_words_prop_3,\n",
    "#                      diff_ent[lang][0], test_loss_0, test_loss_1, test_loss_2, test_loss_3,\n",
    "#                      mi_mis0, mi_mis1, mi_mis2, mi_mis3\n",
    "#                      )\n",
    "\n",
    "print(\"Results have been written to the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_alig_dir = f\"/home/user/ding/Projects/Prosody/precomputed/{model}\"\n",
    "# combine all csv files in mi_alig_dir\n",
    "\n",
    "# List to hold data from all CSV files\n",
    "csv_files = [f for f in os.listdir(mi_alig_dir) if f.endswith(f'{model}.csv')]\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read and store each CSV file as a DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(mi_alig_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame into a new CSV file\n",
    "# combined_csv_path = os.path.join(mi_alig_dir, f'mi_alignment_{model}.csv')\n",
    "# combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "# print(f\"All MI and alignment CSV files have been combined into {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_special_tokens_proportion(dataset):\n",
    "    total_words = 0\n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    count_3 = 0\n",
    "    \n",
    "    # Iterate through all items in the dataset\n",
    "    for i in range(len(dataset)):\n",
    "        # Get each item from the dataset\n",
    "        item = dataset.__getitem__(i)\n",
    "        \n",
    "        input_ids = item['input_ids']  \n",
    "        loss_mask = item['loss_mask']  \n",
    "        word_to_tokens = item[\"word_to_tokens\"]\n",
    "        \n",
    "        # Iterate through only the 2nd, 4th, 6th, etc., elements excluding the last two\n",
    "        for token_ids in word_to_tokens[1:-2:2]:  # Start at index 1 and step by 2, skipping last 2 elements\n",
    "            if isinstance(token_ids, list) and len(token_ids) == 1:\n",
    "                count_1 += 1\n",
    "            if isinstance(token_ids, list) and len(token_ids) == 2:\n",
    "                count_2 += 1\n",
    "            if isinstance(token_ids, list) and len(token_ids) > 2:\n",
    "                count_3 += 1\n",
    "\n",
    "        num_words = sum(1 for mask in loss_mask if mask == 1)\n",
    "        total_words += num_words\n",
    "\n",
    "    \n",
    "    prop_1 = count_1 / total_words\n",
    "    prop_2 = count_2 / total_words\n",
    "    prop_3 = count_3 / total_words\n",
    "    \n",
    "    return total_words, count_1, count_2, count_3, prop_1, prop_2, prop_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_words, train_count_1, train_count_2, train_count_3, train_prop_1, train_prop_2, train_prop_3 = calculate_special_tokens_proportion(dm.train_dataset)\n",
    "dev_total_words, dev_count_1, dev_count_2, dev_count_3, dev_prop_1, dev_prop_2, dev_prop_3 = calculate_special_tokens_proportion(dm.val_dataset)\n",
    "test_total_words, test_count_1, test_count_2, test_count_3, test_prop_1, test_prop_2, test_prop_3 = calculate_special_tokens_proportion(dm.test_dataset)\n",
    "\n",
    "all_total_words = train_total_words + dev_total_words + test_total_words\n",
    "all_count_1 = train_count_1 + dev_count_1 + test_count_1\n",
    "all_count_2 = train_count_2 + dev_count_2 + test_count_2\n",
    "all_count_3 = train_count_3 + dev_count_3 + test_count_3\n",
    "\n",
    "all_prop_1 = all_count_1 / all_total_words\n",
    "all_prop_2 = all_count_2 / all_total_words\n",
    "all_prop_3 = all_count_3 / all_total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tn1 = f\"{PRED_ROOT}/{model}_{lang}_token_n1/metrics.json\"\n",
    "loss_tn2 = f\"{PRED_ROOT}/{model}_{lang}_token_n2/metrics.json\"\n",
    "loss_tn3= f\"{PRED_ROOT}/{model}_{lang}_token_n3/metrics.json\"\n",
    "\n",
    "\n",
    "tn1_loss = get_test_loss(loss_tn1)\n",
    "tn2_loss = get_test_loss(loss_tn2)\n",
    "tn3_loss = get_test_loss(loss_tn3)\n",
    "\n",
    "\n",
    "mi_tn1 = diff_ent[lang][0] - tn1_loss if tn1_loss is not None else None\n",
    "mi_tn2 = diff_ent[lang][0] - tn2_loss if tn2_loss is not None else None\n",
    "mi_tn3 = diff_ent[lang][0] - tn3_loss if tn3_loss is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to /home/user/ding/Projects/Prosody/precomputed/mbert/mi_by_token_type_mbert_yue.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_csv = f\"/home/user/ding/Projects/Prosody/precomputed/{model}/mi_by_token_type_{model}_{lang}.csv\"\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow([\n",
    "        'model', 'language', 'test_total_words',\n",
    "        'token_1_n', 'token_2_n', 'token_3_n',\n",
    "        'token_1_prop', 'token_2_prop', 'token_3_prop',\n",
    "        'loss_token_1', 'loss_token_2', 'loss_token_3',\n",
    "        'diff_ent', 'mi_token_1', 'mi_token_2', 'mi_token_3'\n",
    "    ])\n",
    "    \n",
    "    # Write the data row\n",
    "    writer.writerow([\n",
    "        model, lang, test_total_words,\n",
    "        test_count_1, test_count_2, test_count_3,\n",
    "        format_float(test_prop_1), format_float(test_prop_2), format_float(test_prop_3),\n",
    "        format_float(tn1_loss), format_float(tn2_loss), format_float(tn3_loss),\n",
    "        format_float(diff_ent[lang][0]),\n",
    "        format_float(mi_tn1), format_float(mi_tn2), format_float(mi_tn3)\n",
    "    ])\n",
    "\n",
    "print(f\"Results have been saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_token_dir = f\"/home/user/ding/Projects/Prosody/precomputed/{model}\"\n",
    "# combine all csv files in mi_alig_dir\n",
    "\n",
    "# List to hold data from all CSV files\n",
    "csv_files = [f for f in os.listdir(mi_token_dir) if f.startswith(f'mi_by_token_type_mbert')]\n",
    "print(csv_files)\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read and store each CSV file as a DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(mi_alig_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame into a new CSV file\n",
    "combined_csv_path = os.path.join(mi_alig_dir, f'mi_token_{model}.csv')\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"All MI and alignment CSV files have been combined into {combined_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
