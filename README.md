# Using Information Theory to Characterize Prosodic Typology

This project explores fastText, mGPT, mBERT and a C-KDE way with no LLMs to predict the lexical identity of a word from its prosody.
We then use the information-theoretic measure of entropy to quantify the amount of information that prosody conveys about lexical identity. 

---

## ðŸ”§ Installation & Setup
```bash
conda create --name prosody python=3.10
conda activate prosody
conda install -r requirements.txt
```
Or you can install the requirements with environment.yaml file:
```bash
conda env create -f environment.yaml
```

---
## Data
### Download the already prepared prominence dataset:
```bash
cd data
git clone https://github.com/Helsinki-NLP/prosody.git
```

### Download the Common Voice dataset used in this project:
You can download the Common Voice dataset from the [Common Voice website](https://commonvoice.mozilla.org/en).

### Prepare the Common Voice dataset:
You need to preprocess the Common Voice dataset to extract the prosodic features. You can use the following script to extract the prosodic features:
1. MFA (Montreal Forced Aligner) to align the audio files with the text files: [Tutorial by Eleanor Chodroff](https://lingmethodshub.github.io/content/tools/mfa/mfa-tutorial/), and [the MFA tool](https://montreal-forced-aligner.readthedocs.io/en/latest/).
2. You can extract the prosodic features f0 using the `extract.py` script, and prominence features using the `extract_prominence.py` script. For other prosodic features, you can change the arguments in the `extract.py` script.
Or run the bash script f0_extraction.sh in your server to extract the f0 features.
3. The preprocessed data is also avaiblable in our [OSF repository](https://osf.io/ZMXRT/).

---

## Running the experiments
We followed the models in the project [Quantifying the redundancy between prosody and text](https://github.com/lu-wo/quantifying-redundancy?tab=readme-ov-file) and enhanced the performance by adapting the Conditional Density Estimation (CDE) method in this repository: [CDE](https://github.com/freelunchtheorem/Conditional_Density_Estimation). Please also see the documentation for more information: [CDE Documentation](https://freelunchtheorem.github.io/Conditional_Density_Estimation/docs/html/index.html).

### 1. Model training and conditional entropy estimation
All the bash script for training the models are in the `scripts` folder. In the `baselines` subfolder, you can find the bash scripts for using the fastText models with CDE.
In the `mGPT` subfolder, you can find the bash scripts for using the mGPT models with CDE. In the `mBERT` subfolder, you can find the bash scripts for using the mBERT models with CDE. 
In the `notebooks` folder, you can find the bash scripts for using the C-KDE-all and C-KDE-split models.

### 2. KDE and differential entropy estimation
In the `notebooks` folder, you can find the scripts for using the KDE and Monte Carlo sampling for differential entropy estimation.


---

## Folder Structure
The directory structure of new project looks like this:
```
â”œâ”€â”€ .github                   <- Github Actions workflows
â”‚
â”œâ”€â”€ configs                   <- Hydra configs
â”‚   â”œâ”€â”€ callbacks                <- Callbacks configs
â”‚   â”œâ”€â”€ data                     <- Data configs
â”‚   â”œâ”€â”€ debug                    <- Debugging configs
â”‚   â”œâ”€â”€ experiment               <- Experiment configs
â”‚   â”œâ”€â”€ extras                   <- Extra utilities configs
â”‚   â”œâ”€â”€ hparams_search           <- Hyperparameter search configs
â”‚   â”œâ”€â”€ hydra                    <- Hydra configs
â”‚   â”œâ”€â”€ local                    <- Local configs
â”‚   â”œâ”€â”€ logger                   <- Logger configs
â”‚   â”œâ”€â”€ model                    <- Model configs
â”‚   â”œâ”€â”€ paths                    <- Project paths configs
â”‚   â”œâ”€â”€ trainer                  <- Trainer configs
â”‚   â”‚
â”‚   â”œâ”€â”€ eval.yaml             <- Main config for evaluation
â”‚   â””â”€â”€ train.yaml            <- Main config for training
â”‚
â”œâ”€â”€ data                   <- Project data
â”‚
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”‚
â”œâ”€â”€ notebooks              <- Jupyter notebooks
â”‚
â”œâ”€â”€ r_scripts              <- R scripts for data analysis and visualization
â”‚
â”œâ”€â”€ results                <- Results of the experiments for data analysis and visualization in R
â”‚
â”œâ”€â”€ scripts                <- Shell scripts
â”‚
â”œâ”€â”€ src                    <- Source code
â”‚   â”œâ”€â”€ data                     <- Data scripts
â”‚   â”œâ”€â”€ models                   <- Model scripts
â”‚   â”œâ”€â”€ utils                    <- Utility scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ eval.py                  <- Run evaluation
â”‚   â”œâ”€â”€ train_cv.py              <- Run training with cross-validation
â”‚   â”œâ”€â”€ train.py                 <- Run training
â”‚   â”œâ”€â”€ extraction.py            <- Extract f0 features
â”‚   â””â”€â”€ extraction_prominence.py <- Extract prominence features
â”‚  
â”‚
â”œâ”€â”€ tests                  <- Tests of any kind
â”‚  
â”œâ”€â”€ visualization          <- Visualization figures
â”‚
â”œâ”€â”€ .env.example              <- Example of file for storing private environment variables
â”œâ”€â”€ .gitignore                <- List of files ignored by git
â”œâ”€â”€ .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
â”œâ”€â”€ .project-root             <- File for inferring the position of project root directory
â”œâ”€â”€ environment.yaml          <- File for installing conda environment
â”œâ”€â”€ Makefile                  <- Makefile with commands like `make train` or `make test`
â”œâ”€â”€ pyproject.toml            <- Configuration options for testing and linting
â”œâ”€â”€ requirements.txt          <- File for installing python dependencies
â”œâ”€â”€ setup.py                  <- File for installing project as a package
â””â”€â”€ README.md
```

---
